{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "import configparser\n",
    "import tarfile\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import traceback\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "tar_file = config['DEFAULT']['Tar-File']\n",
    "worktype_file = config['DEFAULT']['Worktype-File']\n",
    "debug = config['DEFAULT']['Debug']\n",
    "key_prefix = config['DEFAULT']['Key-Prefix']\n",
    "test_num = config['DEFAULT']['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='debug.log',level=logging.DEBUG, format='%(asctime)s | %(levelname)s | %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_orcid_work_publication_headers.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    headers= ['key:ID', 'source','local_id','last_updated','url','title','author_list','doi','publication_year','scopus_eid','orcid_type',':LABEL']\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "with open('nodes_orcid_work_dataset_headers.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    headers= ['key:ID', 'source','local_id','last_updated','url','title','doi','publication_year','license','megabyte','orcid_type',':LABEL']\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "with open('nodes_orcid_work_relation_headers.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    headers= ['from_key:START_ID', 'to_uri:END_ID','label:TYPE']\n",
    "    writer.writerow(headers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Tar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Work Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worktype_list = []\n",
    "with open(worktype_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i in reader:\n",
    "        worktype_list.append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root(tar,tar_info):\n",
    "    try:\n",
    "        xml_content = tar.extractfile(tar_info).read().decode('utf-8')\n",
    "        root = ET.fromstring(xml_content)\n",
    "    except Exception as err:\n",
    "        logging.error(\"### Invalid XML file {0}, ERROR:\".format(tar_info.name, str(err)))\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row(file_name,root,namespace_work,namespace_common,worktype_list,key_prefix):\n",
    "\n",
    "    #Set default values to blank\n",
    "    key=source=local_id=last_updated=url=title=author_list=doi=publication_year\\\n",
    "    =scopus_eid=work_type=label=license=megabyte=\"\"\n",
    "\n",
    "    # local_id\n",
    "    local_id = file_name.split('/')[-1]\n",
    "    \n",
    "    # key\n",
    "    key = key_prefix + local_id\n",
    "    \n",
    "    # source\n",
    "    source = \"orcid.org\"\n",
    "    \n",
    "    # last_updated\n",
    "    last_updated = getattr(root.find(namespace_common+\"last-modified-date\"),'text', \"\")\n",
    "    \n",
    "    # url\n",
    "    url = getattr(root.find(namespace_work + \"url\"),'text','')\n",
    "    \n",
    "    # title\n",
    "    for t in root.findall(namespace_work + 'title'):\n",
    "        title = getattr(t.find(namespace_common + 'title'),'text', \"\")\n",
    "    \n",
    "    # author_list\n",
    "    for cs in root.findall(namespace_work + \"contributors\"):\n",
    "        for c in cs.findall(namespace_work + \"contributor\"):\n",
    "            name = getattr(c.find(namespace_work + \"credit-name\"),'text','')\n",
    "            for ca in c.findall(namespace_work + \"contributor-attributes\"):\n",
    "                role = getattr(ca.find(namespace_work + \"contributor-role\"),'text', \"\")\n",
    "                if role == \"author\":\n",
    "                    author_list+=name+\",\"\n",
    "    \n",
    "    # doi, scopus_eid\n",
    "    for eis in root.findall(namespace_common + \"external-ids\"):\n",
    "        for ei in eis.findall(namespace_common + \"external-id\"):\n",
    "            id_type = getattr(ei.find(namespace_common + 'external-id-type'),'text', \"\")\n",
    "            if id_type == \"doi\":\n",
    "                doi = getattr(ei.find(namespace_common + 'external-id-value'),'text', \"\")\n",
    "            elif id_type == \"eid\":\n",
    "                scopus_eid = getattr(ei.find(namespace_common + 'external-id-value'),'text', \"\")\n",
    "    \n",
    "    # publication_year\n",
    "    for d in root.findall(namespace_common + \"publication-date\"):\n",
    "        publication_year = getattr(d.find(namespace_common + \"year\"),'text', \"\")\n",
    "\n",
    "    # orcid type\n",
    "    work_type = getattr(root.find(namespace_work+\"type\"),'text', \"\")\n",
    "    \n",
    "    # label\n",
    "    if work_type in worktype_list:    \n",
    "        label = \"orcid;publication\"\n",
    "    else:\n",
    "        label = \"orcid;dataset\"\n",
    "    \n",
    "    # from_key:START_ID\n",
    "    start_id = key_prefix + file_name.split('/')[-1].split('_')[0]\n",
    "        \n",
    "    return key, source, local_id, last_updated, url, title, author_list, doi,\\\n",
    "publication_year, scopus_eid, work_type, label, license, megabyte, start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_info(start_time, time_count, debug, tar_info, total_count):\n",
    "    if (time.time()-start_time)/600 > time_count:\n",
    "        if debug == \"1\": \n",
    "            logging.info(\"---------------------------------------\")\n",
    "            logging.info(\"The total number of processed xml files: \" + str(total_count))\n",
    "            logging.info(\"The program has been running for \" + str(time_count*10) + \" mins\")\n",
    "            logging.info(\"The current processing file is: \" + tar_info.name)\n",
    "            time_count += 1\n",
    "    return time_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "def write_csv(output_file,row_list):\n",
    "    with open(output_file, 'a+') as f:\n",
    "            writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "start_time = time.time()\n",
    "time_count = 0\n",
    "\n",
    "tar = tarfile.open(tar_file, 'r|gz')\n",
    "for tar_info in tar:\n",
    "    try:\n",
    "        # test mode: limit the number\n",
    "        if test_num:\n",
    "            if total_count>=int(test_num):\n",
    "                break\n",
    "                \n",
    "        if tar_info.isfile():\n",
    "            if tar_info.name.lower().endswith('.xml') \\\n",
    "            and r'/.' not in tar_info.name \\\n",
    "            and r'/works/' in tar_info.name:\n",
    "                \n",
    "                # if debug is 1, then print info every 10mins\n",
    "                time_count = debug_info(start_time, time_count, debug, tar_info, total_count)\n",
    "                                      \n",
    "                namespace_work = '{http://www.orcid.org/ns/work}'\n",
    "                namespace_common = '{http://www.orcid.org/ns/common}'\n",
    "                file_name = os.path.splitext(tar_info.name)[0]\n",
    "                \n",
    "                root = get_root(tar,tar_info)\n",
    "                key, source, local_id, last_updated, url, title, author_list, doi, publication_year, scopus_eid, work_type,\\\n",
    "                label,license, megabyte, start_id= get_row(file_name,root,namespace_work,namespace_common,worktype_list,key_prefix)\n",
    "                \n",
    "                # export to csv \n",
    "                # pulication\n",
    "                if label == \"orcid;publication\":\n",
    "                    row_list = [key, source, local_id, last_updated, url, title, author_list[:-1],\\\n",
    "                                         doi,publication_year, scopus_eid, work_type, label]\n",
    "                    output_file = \"nodes_orcid_work_publication_row.csv\"\n",
    "                    write_csv(output_file,row_list)\n",
    "                # dataset\n",
    "                elif label == \"orcid;dataset\":\n",
    "                    row_list = [key, source, local_id, last_updated, url, title,\\\n",
    "                                         doi,publication_year, license, megabyte, work_type, label]\n",
    "                    output_file = \"nodes_orcid_work_dataset_row.csv\"\n",
    "                    write_csv(output_file,row_list)\n",
    "                # relation\n",
    "                relation_list = [start_id, key, \"relatedTo\"]\n",
    "                rela_output_file = \"nodes_orcid_work_relation_row.csv\"\n",
    "                write_csv(rela_output_file,relation_list)\n",
    "\n",
    "                total_count += 1\n",
    "                \n",
    "    except Exception as err:\n",
    "        logging.error(\"### Error: {0}\".format(str(err)))\n",
    "        logging.error(traceback.format_exc()) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
